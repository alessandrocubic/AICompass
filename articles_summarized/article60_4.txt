Providers or prospective providers conducting testing of high-risk AI systems in real-world conditions outside of regulatory sandboxes must meet several conditions. They are required to develop a real-world testing plan and submit it for approval to the market surveillance authority in the relevant Member State. If the authority does not respond within 30 days, the testing plan is deemed approved, unless local law stipulates otherwise. Registration of the testing in real-world conditions is necessary, with specifics depending on the type of high-risk AI system, and the provider must either be established in the Union or have a legal representative there. Data collected during testing can only be transferred to third countries if appropriate safeguards are in place. The duration of the testing must not exceed six months, extendable by another six with prior notification and justification. Vulnerable groups involved in the testing must be protected, and all participants must be informed about the testing's relevant aspects. Informed consent is required from subjects unless it would impede the testing, in which case their data must be deleted post-testing. Effective oversight of the testing process is mandatory, and the AI system's outputs must be reversible and disregarded if necessary.